{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'AutoencoderMMNet_V1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAutoencoderMMNet_V1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Autoencoder\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDatasetDrone_AllAutoencoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDrone\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchamfer_distance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChamferDistance\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'AutoencoderMMNet_V1'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"..\", \"/app\")))\n",
    "os.chdir(\"/app\")\n",
    "import scipy.io\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from AutoencoderMMNet_V1 import Autoencoder\n",
    "from DatasetDrone_AllAutoencoder import DatasetDrone\n",
    "from chamfer_distance import ChamferDistance\n",
    "from scipy.io import savemat, loadmat\n",
    "from torch_geometric.loader import DataLoader\n",
    "#import config as cfg\n",
    "\n",
    "from Emd.emd_module import emdFunction\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy.io\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txt(path,pred_pcd):\n",
    "    '''\n",
    "    pred_pcd: N by 3\n",
    "    '''\n",
    "    np.savetxt(path + '.txt', pred_pcd, fmt='%.6f')\n",
    "    \n",
    "def emd(p1,p2):\n",
    "    emdist, _ = emdFunction.apply(p1, p2, 0.01, 500)\n",
    "    return torch.sqrt(emdist).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/app/processedData/2025-02-17_11-23-50/visualization/testResult' already exists.\n"
     ]
    }
   ],
   "source": [
    "processedDataFolder_name = os.path.abspath(\"./processedData/2025-02-17_11-23-50/\")\n",
    "matfolder_path = processedDataFolder_name + \"/outputDroneTest\"\n",
    "\n",
    "resultMatFolderPath = processedDataFolder_name + \"/outputDroneAll/\"\n",
    "\n",
    "datasetFolder = \"./datasets/image_data/\"\n",
    "all_files = os.listdir(matfolder_path)\n",
    "\n",
    "# current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# current_datetime = matfolder_path.split(\"/\")[-1]\n",
    "# parent_dir = processedDataFolder_name + \"testResult\"\n",
    "# folder_path = os.path.join(parent_dir, current_datetime)\n",
    "folder_path = processedDataFolder_name + \"/visualization/testResult\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Input vs Ground Truth vs Predicted PCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting .mat files: 100%|██████████| 78/78 [02:45<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "mat_files = [file for file in all_files if file.endswith('.mat')]\n",
    "progressBar = tqdm(mat_files, desc=\"Plotting .mat files\")\n",
    "for mat_file in progressBar:\n",
    "    progressBar.set_postfix(file=mat_file)\n",
    "    file_path = os.path.join(matfolder_path, mat_file)\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "    # print(f\"Loaded {mat_file}\")\n",
    "    # print(\"Keys in the .mat file:\", mat_data.keys())\n",
    "    # for columns in header:\n",
    "        # data = mat_data[columns]\n",
    "        # print(f\"Shape of the {columns}:\", data.shape)\n",
    "\n",
    "    header = ['input', 'pred_pcd', 'gt_pcd', 'Chd', 'EMD']\n",
    "\n",
    "    input_points = mat_data['input']\n",
    "    pred_points = mat_data['pred_pcd']\n",
    "    gt_points = mat_data['gt_pcd']\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    fig.suptitle(f\"Point Cloud Visualization: {mat_file}\", fontsize=16, fontweight='bold')  # Main title\n",
    "\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    scatter1 = ax1.scatter(input_points[:, 0], input_points[:, 1], input_points[:, 2], \n",
    "                        c=input_points[:, 2], cmap='viridis', s=1)\n",
    "    ax1.set_title(\"Input Point Cloud\")\n",
    "    ax1.set_xlabel(\"X\")\n",
    "    ax1.set_ylabel(\"Y\")\n",
    "    ax1.set_zlabel(\"Z\")\n",
    "\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    scatter2 = ax2.scatter(pred_points[:, 0], pred_points[:, 1], pred_points[:, 2], \n",
    "                        c=pred_points[:, 2], cmap='viridis', s=1)\n",
    "    ax2.set_title(\"Predicted Point Cloud\")\n",
    "    ax2.set_xlabel(\"X\")\n",
    "    ax2.set_ylabel(\"Y\")\n",
    "    ax2.set_zlabel(\"Z\")\n",
    "    ax3 = fig.add_subplot(133, projection='3d')\n",
    "    scatter3 = ax3.scatter(gt_points[:, 0], gt_points[:, 1], gt_points[:, 2], \n",
    "                        c=gt_points[:, 2], cmap='viridis', s=1)\n",
    "    ax3.set_title(\"Ground Truth Point Cloud\")\n",
    "    ax3.set_xlabel(\"X\")\n",
    "    ax3.set_ylabel(\"Y\")\n",
    "    ax3.set_zlabel(\"Z\")\n",
    "    # plt.tight_layout()\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "    plt.savefig(f\"{folder_path}/combined_point_clouds_drone_trained_{file_path.split('/')[-1].split('.')[0]}.png\", dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl file saved at: /app/processedData/2025-02-17_11-23-50/rgbImage.csv\n"
     ]
    }
   ],
   "source": [
    "image_data = []\n",
    "for subdir, _, files in os.walk(datasetFolder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):  \n",
    "            file_path = os.path.join(subdir, file)\n",
    "            renamedFile = file[:-4]\n",
    "            renamedFiletimestamp = datetime.strptime(renamedFile, \"%Y-%m-%d_%H_%M_%S_%f\")\n",
    "            renamedFileFormattedTime = renamedFiletimestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\") + \".jpg\"\n",
    "            image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "            date_str, time_hr,time_min,time_sec, microseconds = file[:-4].split(\"_\")\n",
    "            datetime_str = f\"{date_str} {time_hr}:{time_min}:{time_sec}.{microseconds}\"\n",
    "            timestamp = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "            formatted_timestamp = timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "            image_data.append([formatted_timestamp, image, renamedFileFormattedTime, file_path])\n",
    "\n",
    "rgbCsvDF = pd.DataFrame(image_data, columns=[ \"datetime\", \"rgbImage\",\"rgbFilename\", \"rgbFilepath\"])\n",
    "rgbCsvDF[\"datetime\"] = pd.to_datetime(rgbCsvDF[\"datetime\"], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "rgbCsvPath = os.path.join(processedDataFolder_name, \"rgbImage.csv\")\n",
    "rgbCsvDF.to_csv(rgbCsvPath, index=False)\n",
    "rgbCsvDF.to_pickle(processedDataFolder_name + \"/rgbImage.pkl\")\n",
    "print(f\"pkl file saved at: {rgbCsvPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Saved Data and Merge with Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mergedRadarDepthRgb.shape:  (387, 11)\n",
      "100ms - mergedRadarDepthRgb after dropna.shape:  (144, 11)\n",
      "mergedRadarDepthRgb.pkl Exported\n"
     ]
    }
   ],
   "source": [
    "mergedRadarDepth = processedDataFolder_name + \"/mergedRadarDepth.pkl\" \n",
    "mergedRadarDepth = pd.read_pickle(mergedRadarDepth)\n",
    "mergedRadarDepth.reset_index(drop=True, inplace=True)\n",
    "rgbCsvDF = rgbCsvDF.sort_values(by='datetime', ascending=True)\n",
    "mergedRadarDepth = mergedRadarDepth.sort_values(by='datetime', ascending=True)\n",
    "\n",
    "mergedRadarDepthRgb = pd.merge_asof(mergedRadarDepth, rgbCsvDF, on='datetime',tolerance=pd.Timedelta('100ms'), direction='nearest')#change ms\n",
    "print(\"mergedRadarDepthRgb.shape: \",mergedRadarDepthRgb.shape)\n",
    "mergedRadarDepthRgb =mergedRadarDepthRgb.dropna(subset=['rgbFilename'])\n",
    "print(\"100ms - mergedRadarDepthRgb after dropna.shape: \",mergedRadarDepthRgb.shape)\n",
    "\n",
    "mergedRadarDepthRgb.to_csv(processedDataFolder_name + \"/mergedRadarDepthRgb.csv\", index=False)\n",
    "mergedRadarDepthRgb.to_pickle(processedDataFolder_name + \"/mergedRadarDepthRgb.pkl\")\n",
    "print(\"mergedRadarDepthRgb.pkl Exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export ALl Data as MAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All MAT file will be saved in /app/processedData/2025-02-17_11-23-50/droneData_All/processedData/\n",
      "Folder '/app/processedData/2025-02-17_11-23-50/droneData_All/processedData/' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving All Data: 100%|██████████| 144/144 [00:04<00:00, 28.87it/s, file=2025-01-29 12:43:23.400000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 144 testing .mat files to '/app/processedData/2025-02-17_11-23-50/droneData_All/processedData/' and recorded in '/app/processedData/2025-02-17_11-23-50/droneData_All/datalist.txt'.\n"
     ]
    }
   ],
   "source": [
    "#exporting the pkl for test on slidesr\n",
    "outputDirAll = processedDataFolder_name + \"/droneData_All/processedData/\"  \n",
    "txt_file_All = processedDataFolder_name + \"/droneData_All/datalist.txt\" \n",
    "print(f\"All MAT file will be saved in {outputDirAll}\")\n",
    "if not os.path.exists(outputDirAll):\n",
    "    os.makedirs(outputDirAll)\n",
    "    print(f\"Folder '{outputDirAll}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{outputDirAll}' already exists.\")\n",
    "\n",
    "if True:\n",
    "    with open(txt_file_All, \"w\") as all_out:\n",
    "        i=0     \n",
    "        progressBar = tqdm(mergedRadarDepthRgb.iterrows(), desc=\"Saving All Data\", total=len(mergedRadarDepthRgb))\n",
    "        for idx,row in progressBar:\n",
    "            # row = mergedPcdDepthRgb.iloc[idx]\n",
    "            # mat_file_name = f\"{idx + 1}_mmwave.mat\"\n",
    "\n",
    "            # #naming the matfile with repective image name\n",
    "            # timestampStr, fTimestampStr = mergedRadarDepthRgb[\"rgbFilename\"][idx].split(\".\")[:-1]\n",
    "            # matName = f\"{timestampStr}.{fTimestampStr}\"\n",
    "\n",
    "            matName = mergedRadarDepthRgb['datetime'][idx]\n",
    "\n",
    "            progressBar.set_postfix(file=matName)\n",
    "\n",
    "            mat_file_name = f\"{matName}_mmwave.mat\"\n",
    "            mat_file_path = os.path.join(outputDirAll, mat_file_name)\n",
    "\n",
    "            savemat(mat_file_path, {\n",
    "                'radarPCD': mergedRadarDepthRgb['radarPCD'][idx],\n",
    "                'depthPCD': mergedRadarDepthRgb['depthPCD'][idx],\n",
    "                'datetime': mergedRadarDepthRgb['datetime'][idx]\n",
    "            })\n",
    "            all_out.write(mat_file_path + \"\\n\")\n",
    "    print(f\"Exported {len(mergedRadarDepthRgb)} testing .mat files to '{outputDirAll}' and recorded in '{txt_file_All}'.\")\n",
    "\n",
    "with open(txt_file_All, \"r\") as f:\n",
    "    mat_file_paths = [line.strip() for line in f.readlines() if line.strip().endswith(\".mat\")]\n",
    "\n",
    "mat_filenames = [path.split(\"/\")[-1] for path in mat_file_paths]\n",
    "mat_filenames_array = np.array(mat_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on ALl Mat File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/processedData/2025-02-17_11-23-50/dronetrained/checkpoints/2025-03-17T07:58:51.569265/MMNet_ChBest.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = processedDataFolder_name + '/dronetrained/checkpoints/2025-03-17T07:58:51.569265/MMNet_ChBest.pt'#'./trained/MMNet_ChBest.pt'\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"decoder4.dg1.conv.nn.0.0.weight\", \"decoder4.dg1.conv.nn.0.0.bias\", \"decoder4.dg1.conv.nn.1.0.weight\", \"decoder4.dg1.conv.nn.1.0.bias\", \"decoder4.ps.weight\", \"decoder4.mlp_1.mlp.0.weight\", \"decoder4.mlp_1.mlp.0.bias\", \"decoder4.mlp_1.mlp.2.weight\", \"decoder4.mlp_1.mlp.2.bias\", \"decoder4.mlp_2.mlp.0.weight\", \"decoder4.mlp_2.mlp.0.bias\", \"decoder4.mlp_2.mlp.2.weight\", \"decoder4.mlp_2.mlp.2.bias\", \"decoder3.dg1.conv.nn.0.0.weight\", \"decoder3.dg1.conv.nn.0.0.bias\", \"decoder3.dg1.conv.nn.1.0.weight\", \"decoder3.dg1.conv.nn.1.0.bias\", \"decoder3.ps.weight\", \"decoder3.mlp_1.mlp.0.weight\", \"decoder3.mlp_1.mlp.0.bias\", \"decoder3.mlp_1.mlp.2.weight\", \"decoder3.mlp_1.mlp.2.bias\", \"decoder3.mlp_2.mlp.0.weight\", \"decoder3.mlp_2.mlp.0.bias\", \"decoder3.mlp_2.mlp.2.weight\", \"decoder3.mlp_2.mlp.2.bias\", \"pdPointsDownsample.pd_points_downsample.weight\", \"pdPointsDownsample.pd_points_downsample.bias\", \"pdPointsDownsample.fc.weight\", \"pdPointsDownsample.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m ChD \u001b[38;5;241m=\u001b[39m ChamferDistance()  \u001b[38;5;66;03m# chamfer loss for \u001b[39;00m\n\u001b[1;32m     10\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path,map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGen_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\u001b[39;00m\n\u001b[1;32m     15\u001b[0m timeDir \u001b[38;5;241m=\u001b[39m processedDataFolder_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tUnexpected key(s) in state_dict: \"decoder4.dg1.conv.nn.0.0.weight\", \"decoder4.dg1.conv.nn.0.0.bias\", \"decoder4.dg1.conv.nn.1.0.weight\", \"decoder4.dg1.conv.nn.1.0.bias\", \"decoder4.ps.weight\", \"decoder4.mlp_1.mlp.0.weight\", \"decoder4.mlp_1.mlp.0.bias\", \"decoder4.mlp_1.mlp.2.weight\", \"decoder4.mlp_1.mlp.2.bias\", \"decoder4.mlp_2.mlp.0.weight\", \"decoder4.mlp_2.mlp.0.bias\", \"decoder4.mlp_2.mlp.2.weight\", \"decoder4.mlp_2.mlp.2.bias\", \"decoder3.dg1.conv.nn.0.0.weight\", \"decoder3.dg1.conv.nn.0.0.bias\", \"decoder3.dg1.conv.nn.1.0.weight\", \"decoder3.dg1.conv.nn.1.0.bias\", \"decoder3.ps.weight\", \"decoder3.mlp_1.mlp.0.weight\", \"decoder3.mlp_1.mlp.0.bias\", \"decoder3.mlp_1.mlp.2.weight\", \"decoder3.mlp_1.mlp.2.bias\", \"decoder3.mlp_2.mlp.0.weight\", \"decoder3.mlp_2.mlp.0.bias\", \"decoder3.mlp_2.mlp.2.weight\", \"decoder3.mlp_2.mlp.2.bias\", \"pdPointsDownsample.pd_points_downsample.weight\", \"pdPointsDownsample.pd_points_downsample.bias\", \"pdPointsDownsample.fc.weight\", \"pdPointsDownsample.fc.bias\". "
     ]
    }
   ],
   "source": [
    "test_dataset = DatasetDrone(processedDataFolder_name + '/droneData_All', split='test')\n",
    "print(len(test_dataset))\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, follow_batch=['y', 'x'],shuffle=False,drop_last=False)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "G = Autoencoder(device).to(device)\n",
    "\n",
    "ChD = ChamferDistance()  # chamfer loss for \n",
    "\n",
    "checkpoint = torch.load(model_path,map_location=device)\n",
    "G.load_state_dict(checkpoint['Gen_state_dict'])\n",
    "\n",
    "\n",
    "# current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "timeDir = processedDataFolder_name.split(\"/\")[-2]\n",
    "#enable it when multiple test scenios required\n",
    "# folder_path = os.path.join(resultMatFolderPath, timeDir)\n",
    "folder_path = resultMatFolderPath\n",
    "os.makedirs(folder_path,exist_ok=True)\n",
    "\n",
    "G.eval()\n",
    "step = 0\n",
    "# print ('Valid: ')\n",
    "loss_g =0\n",
    "each_chd = []\n",
    "each_emd = []\n",
    "each_chdEncoded = []\n",
    "each_emdEncoded = []\n",
    "progressBar = tqdm(test_data_loader, desc=\"Testing Progress\")\n",
    "\n",
    "for data in progressBar:\n",
    "    # print(\"data shape: \", len(data))\n",
    "    file_name = mat_filenames_array[step]\n",
    "    progressBar.set_postfix(file=file_name)\n",
    "    data =data.to(device)\n",
    "    # 1. Test G \n",
    "    gt_pcd = data.y     # 10000 by 3\n",
    "    #x_fft = data.fft   #N:9000 M:256 H:4  W:3\n",
    "    x_pos = data.x   #N:9000 M:3\n",
    "    #x_img = data.imgs   #N:  M: 1   H: 80 W:80\n",
    "    #x_ang = data.ang      #N:9000 M:3\n",
    "    x_ini = data.ini\n",
    "    batch_size = torch.max(data.y_batch)+1\n",
    "    # print(x_ini.shape,x_pos.shape,gt_pcd.shape) #torch.Size([1, 3]) torch.Size([1024, 3]) torch.Size([2048, 3])\n",
    "    Max=1e17;Min=0\n",
    "    x_ini=(x_ini-Max)/(Max-Min)\n",
    "    score,ini_points,pred,pred_Doppler,scoreEncoded,pd_pointsEncoded,predDopplerEncoded,ini_pointsEncoded= G(x_ini,x_pos,data.x_batch)\n",
    "    dist1, dist2, idx1, idx2 = ChD(pred, gt_pcd.view(batch_size,-1,3))  # test G \n",
    "    dist1Encoded, dist2Encoded, idx1Encoded, idx2Encoded = ChD(pd_pointsEncoded, x_pos.view(batch_size,-1,3))  # test G \n",
    "\n",
    "    g_error = 0.5*(torch.mean(torch.sqrt(dist1))) + 0.5*(torch.mean(torch.sqrt(dist2)))\n",
    "    #print(g_error.size())\n",
    "    g_errorEncoded = 0.5*(torch.mean(torch.sqrt(dist1Encoded))) + 0.5*(torch.mean(torch.sqrt(dist2Encoded)))\n",
    "\n",
    "    loss_g += g_error.item()\n",
    "    loss_gEncoded += g_errorEncoded.item()\n",
    "    emd_error = emd(pred,gt_pcd.view(batch_size,-1,3))\n",
    "    emd_errorEncoded = emd(pd_pointsEncoded,x_pos.view(batch_size,-1,3))\n",
    "        \n",
    "    predDopplerEncoded = predDopplerEncoded.squeeze(-1)\n",
    "    loss_dopplerMse = F.mse_loss(predDopplerEncoded,x_ini)\n",
    "    gen_data = {\n",
    "    'input': x_pos.cpu().numpy().reshape((-1,3)),\n",
    "    'pred_pcd': pred.detach().cpu().numpy().reshape((-1,3)),\n",
    "    'pred_pcdDecoded': pd_pointsEncoded.detach().cpu().numpy().reshape((-1,3)),\n",
    "    'gt_pcd': gt_pcd.cpu().numpy().reshape((-1,3)),\n",
    "    'predDoppler': pred_Doppler.detach().cpu().numpy().reshape((-1,1)),\n",
    "    'predDopplerDecoded': predDopplerEncoded.detach().cpu().numpy().reshape((-1,1)),\n",
    "    'Chd':g_error.item(),\n",
    "    'EMD':emd_error.item(),\n",
    "    'ChdEncoded':g_errorEncoded.item(),\n",
    "    'EMDEncoded':emd_errorEncoded.item(),\n",
    "    'LossDoppler':loss_dopplerMse.item()\n",
    "    }\n",
    "    each_chd.append(g_error.item())\n",
    "    each_emd.append(emd_error.item())\n",
    "    each_chdEncoded.append(g_errorEncoded.item())\n",
    "    each_emdEncoded.append(emd_errorEncoded.item())\n",
    "    \n",
    "    savemat(folder_path + f\"/{mat_filenames_array[step]}\", gen_data)\n",
    "    step = step + 1\n",
    "print(\"loss_g/len(test_dataset): \",loss_g/len(test_dataset))\n",
    "save_txt(folder_path + \"/chd_loss.txt\",np.array(each_chd))\n",
    "save_txt(folder_path + \"/emd_loss.txt\",np.array(each_emd))\n",
    "save_txt(folder_path + \"/chd_lossEncoded.txt\",np.array(each_chdEncoded))\n",
    "save_txt(folder_path + \"/emd_lossEncoded.txt\",np.array(each_emdEncoded))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(resultMatFolderPath)\n",
    "folder_path = processedDataFolder_name + \"visualization/testResultAll\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files = [file for file in all_files if file.endswith('.mat')]\n",
    "progressBar = tqdm(mat_files, desc=\"Plotting .mat files\")\n",
    "# sys.exit(0)\n",
    "predictedPcd = []\n",
    "\n",
    "for mat_file in progressBar:\n",
    "    progressBar.set_postfix(file=mat_file)\n",
    "    file_path = os.path.join(resultMatFolderPath, mat_file)\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "    # print(f\"Loaded {mat_file}\")\n",
    "    # print(\"Keys in the .mat file:\", mat_data.keys())\n",
    "    # for columns in header:\n",
    "        # data = mat_data[columns]\n",
    "        # print(f\"Shape of the {columns}:\", data.shape)\n",
    "\n",
    "    header = ['input', 'pred_pcd', 'gt_pcd', 'Chd', 'EMD']\n",
    "\n",
    "    input_points = mat_data['input']\n",
    "    pred_points = mat_data['pred_pcd']\n",
    "    gt_points = mat_data['gt_pcd']\n",
    "\n",
    "    predictedPcd.append(pred_points)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    fig.suptitle(f\"Point Cloud Visualization: {mat_file}\", fontsize=7, fontweight='bold')  # Main title\n",
    "\n",
    "    ax1 = fig.add_subplot(141, projection='3d')\n",
    "    scatter1 = ax1.scatter(input_points[:, 0], input_points[:, 1], input_points[:, 2], \n",
    "                        c=input_points[:, 2], cmap='viridis', s=1)\n",
    "    ax1.set_title(\"Input Point Cloud\")\n",
    "    ax1.set_xlabel(\"X\")\n",
    "    ax1.set_ylabel(\"Y\")\n",
    "    ax1.set_zlabel(\"Z\")\n",
    "\n",
    "    ax2 = fig.add_subplot(142, projection='3d')\n",
    "    scatter2 = ax2.scatter(pred_points[:, 0], pred_points[:, 1], pred_points[:, 2], \n",
    "                        c=pred_points[:, 2], cmap='viridis', s=1)\n",
    "    ax2.set_title(\"Predicted Point Cloud\")\n",
    "    ax2.set_xlabel(\"X\")\n",
    "    ax2.set_ylabel(\"Y\")\n",
    "    ax2.set_zlabel(\"Z\")\n",
    "    ax3 = fig.add_subplot(143, projection='3d')\n",
    "    scatter3 = ax3.scatter(gt_points[:, 0], gt_points[:, 1], gt_points[:, 2], \n",
    "                        c=gt_points[:, 2], cmap='viridis', s=1)\n",
    "    ax3.set_title(\"Ground Truth Point Cloud\")\n",
    "    ax3.set_xlabel(\"X\")\n",
    "    ax3.set_ylabel(\"Y\")\n",
    "    ax3.set_zlabel(\"Z\")\n",
    "\n",
    "    ax4 = fig.add_subplot(144)\n",
    "\n",
    "\n",
    "    timestampStr = mat_file.split(\"_\")[0]\n",
    "\n",
    "    # timestampStr = datetime.strptime(timestampStr, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    # rgbFileNa = timestampStr.strftime(\"%Y-%m-%d %H:%M:%S.%f\") + \".jpg\"\n",
    "    # print(rgbFileNa)\n",
    "\n",
    "    rgbFilePt = mergedRadarDepthRgb.loc[mergedRadarDepthRgb['datetime'] == timestampStr, 'rgbFilepath']\n",
    "    # print(f\"{mat_file} {timestampStr} {rgbFilePt}\" )\n",
    "    if not rgbFilePt.empty:\n",
    "        rgbFilePt = rgbFilePt.iloc[0] \n",
    "    else:\n",
    "        rgbFilePt = None\n",
    "    \n",
    "    img = Image.open(rgbFilePt)\n",
    "\n",
    "    ax4.imshow(img)  \n",
    "    ax4.set_title(\"RGB Image\")\n",
    "    ax4.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "    plt.savefig(f\"{folder_path}/{mat_file}.png\", dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedRadarDepthRgb[\"predPCD\"] = predictedPcd\n",
    "mergedRadarDepthRgb.to_csv(processedDataFolder_name + \"mergedRadarDepthRgbPred.csv\", index=False)\n",
    "mergedRadarDepthRgb.to_pickle(processedDataFolder_name + \"mergedRadarDepthRgbPred.pkl\")\n",
    "print(\"mergedRadarDepthRgbPred.pkl Exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
